{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention 自然语言处理从零开始：使用序列到序列网络和注意力机制进行翻译\n",
    "===============================================================================\n",
    "\n",
    "**Author**: [Sean Robertson](https://github.com/spro)\n",
    "\n",
    "This is the third and final tutorial on doing \\\"NLP From Scratch\\\",\n",
    "where we write our own classes and functions to preprocess the data to\n",
    "do our NLP modeling tasks. We hope after you complete this tutorial that\n",
    "you\\'ll proceed to learn how [torchtext]{.title-ref} can handle much of\n",
    "this preprocessing for you in the three tutorials immediately following\n",
    "this one.\n",
    "\n",
    "这是“从零开始做自然语言处理”的第三个也是最后一个教程，在这个教程中，我们编写自己的类和函数来预处理数据，以完成我们的自然语言处理建模任务。我们希望在你完成本教程后，可以继续学习如何使用[torchtext]{.title-ref}来处理这些预处理工作，这将在紧接着的三个教程中进行介绍。\n",
    "\n",
    "In this project we will be teaching a neural network to translate from\n",
    "French to English.\n",
    "\n",
    "在这个项目中，我们将教一个神经网络从法语翻译到英语。\n",
    "\n",
    "``` {.sourceCode .sh}\n",
    "[KEY: > input, = target, < output]\n",
    "\n",
    "> il est en train de peindre un tableau .\n",
    "= he is painting a picture .\n",
    "< he is painting a picture .\n",
    "\n",
    "> pourquoi ne pas essayer ce vin delicieux ?\n",
    "= why not try that delicious wine ?\n",
    "< why not try that delicious wine ?\n",
    "\n",
    "> elle n est pas poete mais romanciere .\n",
    "= she is not a poet but a novelist .\n",
    "< she not not a poet but a novelist .\n",
    "\n",
    "> vous etes trop maigre .\n",
    "= you re too skinny .\n",
    "< you re all alone .\n",
    "```\n",
    "\n",
    "\\... to varying degrees of success.\n",
    "\n",
    "...以不同程度的成功实现。\n",
    "\n",
    "This is made possible by the simple but powerful idea of the [sequence\n",
    "to sequence network](https://arxiv.org/abs/1409.3215), in which two\n",
    "recurrent neural networks work together to transform one sequence to\n",
    "another. An encoder network condenses an input sequence into a vector,\n",
    "and a decoder network unfolds that vector into a new sequence.\n",
    "\n",
    "这是通过一个简单但强大的想法实现的，即[序列到序列网络](https://arxiv.org/abs/1409.3215)，其中两个循环神经网络协同工作，将一个序列转换为另一个序列。编码器网络将输入序列压缩成一个向量，而解码器网络将该向量展开成一个新的序列。\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/seq2seq.png)\n",
    "\n",
    "To improve upon this model we\\'ll use an [attention\n",
    "mechanism](https://arxiv.org/abs/1409.0473), which lets the decoder\n",
    "learn to focus over a specific range of the input sequence.\n",
    "\n",
    "为了改进这个模型，我们将使用[注意力机制](https://arxiv.org/abs/1409.0473)，这让解码器能够学习在输入序列的特定范围内集中注意力。\n",
    "\n",
    "**Recommended Reading:**\n",
    "\n",
    "I assume you have at least installed PyTorch, know Python, and\n",
    "understand Tensors:\n",
    "\n",
    "-   <https://pytorch.org/> For installation instructions\n",
    "-   `/beginner/deep_learning_60min_blitz`{.interpreted-text role=\"doc\"}\n",
    "    to get started with PyTorch in general\n",
    "-   `/beginner/pytorch_with_examples`{.interpreted-text role=\"doc\"} for\n",
    "    a wide and deep overview\n",
    "-   `/beginner/former_torchies_tutorial`{.interpreted-text role=\"doc\"}\n",
    "    if you are former Lua Torch user\n",
    "\n",
    "我假设你至少已经安装了PyTorch，懂得Python，并理解张量：\n",
    "\n",
    "- [https://pytorch.org/](https://pytorch.org/) 安装说明\n",
    "    \n",
    "- [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) 开始学习PyTorch的入门教程\n",
    "    \n",
    "- [Learning PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) 广泛而深入的概述\n",
    "    \n",
    "- [PyTorch for Former Torch Users](https://pytorch.org/tutorials/beginner/former_torchies_tutorial.html) 如果你以前是Lua Torch用户\n",
    "\n",
    "It would also be useful to know about Sequence to Sequence networks and\n",
    "how they work:\n",
    "\n",
    "-   [Learning Phrase Representations using RNN Encoder-Decoder for\n",
    "    Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
    "-   [Sequence to Sequence Learning with Neural\n",
    "    Networks](https://arxiv.org/abs/1409.3215)\n",
    "-   [Neural Machine Translation by Jointly Learning to Align and\n",
    "    Translate](https://arxiv.org/abs/1409.0473)\n",
    "-   [A Neural Conversational Model](https://arxiv.org/abs/1506.05869)\n",
    "\n",
    "了解序列到序列网络及其工作原理也很有用：\n",
    "\n",
    "- [使用RNN编码器-解码器进行统计机器翻译的短语表示学习](https://arxiv.org/abs/1406.1078)\n",
    "- [使用神经网络进行序列到序列学习](https://arxiv.org/abs/1409.3215)\n",
    "- [通过联合学习对齐和翻译进行神经机器翻译](https://arxiv.org/abs/1409.0473)\n",
    "- [神经会话模型](https://arxiv.org/abs/1506.05869)\n",
    "\n",
    "You will also find the previous tutorials on [NLP From Scratch: Classifying Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) and [NLP From Scratch: Generating Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html) helpful as those concepts are very similar to the Encoder and Decoder models, respectively.\n",
    "\n",
    "你还会发现之前的教程也很有帮助，例如[NLP From Scratch: Classifying Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)和[NLP From Scratch: Generating Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)，因为这些概念分别与编码器和解码器模型非常相似。\n",
    "\n",
    "**Requirements**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "# 这行代码使用了 __future__ 模块中的一些功能，以确保在使用 Python 2 时启用 Python 3 中的特性。这是为了帮助在 Python 2 中逐步迁移到 Python 3。具体来说，这段代码启用了三个 Python 3 的特性：\n",
    "#   unicode_literals：启用此功能后，所有的字符串字面量默认都是 Unicode 字符串，而不是字节字符串。这与 Python 3 的行为一致，在 Python 2 中默认字符串字面量是字节字符串。\n",
    "#   print_function：启用此功能后，print 语句将作为一个函数使用。这意味着你需要使用 print() 而不是 print 语句。这与 Python 3 的行为一致，在 Python 3 中 print 是一个函数，而不是一个语句。\n",
    "#   division：启用此功能后，除法运算符 / 将执行浮点除法（即使两个操作数都是整数），而不是执行整数除法。这与 Python 3 的行为一致，在 Python 3 中 / 总是执行浮点除法，而 // 才是执行整数除法。\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data files\n",
    "==================\n",
    "\n",
    "The data for this project is a set of many thousands of English to\n",
    "French translation pairs.\n",
    "\n",
    "本项目的数据是一组包含数千个英法翻译对的集合。\n",
    "\n",
    "[This question on Open Data Stack\n",
    "Exchange](https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages)\n",
    "pointed me to the open translation site <https://tatoeba.org/> which has\n",
    "downloads available at <https://tatoeba.org/eng/downloads> - and better\n",
    "yet, someone did the extra work of splitting language pairs into\n",
    "individual text files here: <https://www.manythings.org/anki/>\n",
    "\n",
    "[Open Data Stack Exchange上的这个问题](https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages) 指引我找到了开放翻译网站 [https://tatoeba.org/](https://tatoeba.org/)，其下载地址为 [https://tatoeba.org/eng/downloads](https://tatoeba.org/eng/downloads)。更好的是，有人做了额外的工作，将语言对拆分成单独的文本文件，地址是 [https://www.manythings.org/anki/](https://www.manythings.org/anki/)\n",
    "\n",
    "The English to French pairs are too big to include in the repository, so\n",
    "download to `data/eng-fra.txt` before continuing. The file is a tab\n",
    "separated list of translation pairs:\n",
    "\n",
    "由于英法对的文件太大，无法包含在代码仓库中，因此请在继续之前下载到 `data/eng-fra.txt`。该文件是一个用制表符分隔的翻译对列表：\n",
    "\n",
    "``` {.sourceCode .sh}\n",
    "I am cold.    J'ai froid.\n",
    "```\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p>Download the data from<a href=\"https://download.pytorch.org/tutorial/data.zip\">here</a>and extract it to the current directory.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the character encoding used in the character-level RNN\n",
    "tutorials, we will be representing each word in a language as a one-hot\n",
    "vector, or giant vector of zeros except for a single one (at the index\n",
    "of the word). Compared to the dozens of characters that might exist in a\n",
    "language, there are many many more words, so the encoding vector is much\n",
    "larger. We will however cheat a bit and trim the data to only use a few\n",
    "thousand words per language.\n",
    "\n",
    "与字符级RNN教程中使用的字符编码类似，我们将用独热向量（one-hot vector）表示语言中的每个单词，也就是除了单个位置为1，其余全为0的巨型向量。相比于一种语言中可能存在的几十个字符，单词数量要多得多，所以编码向量会大很多。不过，我们会稍微简化一下数据，每种语言只使用几千个单词。\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/word-encoding.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We\\'ll need a unique index per word to use as the inputs and targets of\n",
    "the networks later. To keep track of all this we will use a helper class\n",
    "called `Lang` which has word → index (`word2index`) and index → word\n",
    "(`index2word`) dictionaries, as well as a count of each word\n",
    "`word2count` which will be used to replace rare words later.\n",
    "\n",
    "我们需要为每个单词分配一个唯一的索引，以便在后续用作网络的输入和目标。为了跟踪所有这些信息，我们将使用一个名为`Lang`的辅助类，该类包含单词到索引（`word2index`）和索引到单词（`index2word`）的字典，以及每个单词的计数（`word2count`），后者将用于替换罕见单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are all in Unicode, to simplify we will turn Unicode\n",
    "characters to ASCII, make everything lowercase, and trim most\n",
    "punctuation.\n",
    "\n",
    "这些文件都是Unicode编码的，为了简化处理，我们将把Unicode字符转换为ASCII字符，把所有内容变成小写，并删除大部分标点符号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        # 'Mn' 表示非间隔标记（通常是重音符号等）\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # s.lower() 将字符串 s 转换为小写。\n",
    "    # s.strip() 去除字符串 s 两端的多余空格。\n",
    "    # unicodeToAscii(s) 调用前面定义的 unicodeToAscii 函数，将 Unicode 字符串转换为 ASCII 字符串。\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # 使用正则表达式 re.sub 在标点符号（.、!、?）前添加一个空格。\n",
    "    # 例如，将 \"Hello!\" 转换为 \"Hello !\", \"What?\" 转换为 \"What ?\"。\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    # 使用正则表达式 re.sub 将所有非字母字符（除了 .、! 和 ?）替换为空格。\n",
    "    # 例如，将 \"Hello, world!\" 转换为 \"Hello world !\", \"123 Go!\" 转换为 \" Go !\"\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caf \n",
      "cafe\n",
      "['H', 'e', '́', 'l', 'l', 'o', ',', ' ', 'w', 'o', '̈', 'r', 'l', 'd', '!', ' ', 'C', 'a', 'f', 'e', '́']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, world! Cafe'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(re.sub(r\"[^a-zA-Z!?]+\", r\" \", \"Café\"))\n",
    "print(normalizeString(\"Café\"))\n",
    "s = \"Héllo, wörld! Café\"\n",
    "print([c for c in unicodedata.normalize('NFD', s)])\n",
    "unicodeToAscii(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split\n",
    "lines into pairs. The files are all English → Other Language, so if we\n",
    "want to translate from Other Language → English I added the `reverse`\n",
    "flag to reverse the pairs.\n",
    "\n",
    "为了读取数据文件，我们将把文件按行分割，然后再将每行分割成对。这些文件都是从英语翻译到其他语言的，所以如果我们想要从其他语言翻译到英语，可以添加`reverse`标志来反转这些对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    # 这段代码定义了一个名为 readLangs 的函数，用于读取两种语言的平行语料文件，将每一行文本拆分为句子对，并对句子进行规范化处理。该函数还支持反转句子对的顺序，并创建语言实例对象\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # 使用 open 函数打开文件 'data/%s-%s.txt' % (lang1, lang2)，文件路径根据 lang1 和 lang2 动态生成。\n",
    "    # 指定编码为 utf-8 以正确处理 Unicode 字符。\n",
    "    # 使用 read 方法读取整个文件内容，并使用 strip 方法去除首尾空白。\n",
    "    # 使用 split('\\n') 方法将文件内容按行分割成列表 lines。\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    # 每一行 l 通常包含两个句子，用制表符 \\t 分隔, split('\\t') 将行 l 按制表符拆分成两个子字符串（句子）\n",
    "    # lines = [\n",
    "    #             \"I am a student.\\tJe suis un étudiant.\",\n",
    "    #             \"How are you?\\tComment ça va?\"\n",
    "    #         ]\n",
    "    # pairs = [\n",
    "    #             ['i am a student .', 'je suis un etudiant .'],\n",
    "    #             ['how are you ?', 'comment ca va ?']\n",
    "    #         ]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "    # lang1 = 'eng', lang2 = 'fra'\n",
    "    # pairs = [\n",
    "    #             ['i am a student .', 'je suis un etudiant .'],\n",
    "    #             ['how are you ?', 'comment ca va ?']\n",
    "    #         ]\n",
    "    # reverse = True:\n",
    "    # pairs = [\n",
    "    #             ['je suis un etudiant .', 'i am a student .'],\n",
    "    #             ['comment ca va ?', 'how are you ?']\n",
    "    #         ]\n",
    "    # input_lang = Lang('fra')\n",
    "    # output_lang = Lang('eng')\n",
    "    # reverse = False:\n",
    "    # input_lang = Lang('eng')\n",
    "    # output_lang = Lang('fra')\n",
    "    \n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are a *lot* of example sentences and we want to train\n",
    "something quickly, we\\'ll trim the data set to only relatively short and\n",
    "simple sentences. Here the maximum length is 10 words (that includes\n",
    "ending punctuation) and we\\'re filtering to sentences that translate to\n",
    "the form \\\"I am\\\" or \\\"He is\\\" etc. (accounting for apostrophes replaced\n",
    "earlier).\n",
    "\n",
    "由于有*大量*的示例句子，我们希望快速训练，因此我们会将数据集修剪到仅包含相对较短和简单的句子。在这里，最大长度为10个单词（包括结束标点符号），并且我们过滤出翻译成“我在”或“他是”等形式的句子（考虑到前面已替换的撇号）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "    # p[1] 是句子对中的第二个句子（通常是目标语言句子）\n",
    "    # p[1].split(' ') 将句子 p[1] 按空格拆分成单词列表\n",
    "    # len(p[1].split(' ')) 计算句子中的单词数\n",
    "    # p[1].startswith(eng_prefixes) 检查句子 p[1] 是否以 eng_prefixes 中的某个前缀开头\n",
    "    # eng-fra加上reverse=True所以检查p[1]\n",
    "    \n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "pair1 = [\"je suis un etudiant\", \"i am a student\"]\n",
    "pair2 = [\"c est une longue phrase qui dépasse la longueur maximale\", \"this is a long sentence that exceeds the maximum length\"]\n",
    "pair3 = [\"il est ici\", \"he is here\"]\n",
    "pair4 = [\"nous sommes heureux\", \"we are happy\"]\n",
    "pair5 = [\"i am happy\", \"i am\"]\n",
    "\n",
    "print(filterPair(pair1))\n",
    "print(filterPair(pair2))\n",
    "print(filterPair(pair3))\n",
    "print(filterPair(pair4))\n",
    "print(filterPair(pair5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-   Read text file and split into lines, split lines into pairs\n",
    "-   Normalize text, filter by length and content\n",
    "-   Make word lists from sentences in pairs\n",
    "\n",
    "准备数据的完整过程是：\n",
    "\n",
    "- 读取文本文件并按行分割，将行分割成对\n",
    "- 规范化文本，根据长度和内容进行过滤\n",
    "- 从对中的句子中制作单词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n",
      "['elles ne sont pas seules', 'they aren t alone']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "循环神经网络（RNN）是一种对序列进行操作的网络，它使用自身的输出作为后续步骤的输入。\n",
    "\n",
    "A [Sequence to Sequence network](https://arxiv.org/abs/1409.3215), or\n",
    "seq2seq network, or [Encoder Decoder\n",
    "network](https://arxiv.org/pdf/1406.1078v3.pdf), is a model consisting\n",
    "of two RNNs called the encoder and decoder. The encoder reads an input\n",
    "sequence and outputs a single vector, and the decoder reads that vector\n",
    "to produce an output sequence.\n",
    "\n",
    "[序列到序列网络](https://arxiv.org/abs/1409.3215)（seq2seq网络）或[编码器解码器网络](https://arxiv.org/pdf/1406.1078v3.pdf)是一种由两个RNN组成的模型，分别称为编码器和解码器。编码器读取输入序列并输出一个单一的向量，解码器读取该向量以生成输出序列。\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/seq2seq.png)\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "与使用单个RNN进行序列预测（每个输入对应一个输出）不同，seq2seq模型使我们摆脱了序列长度和顺序的限制，这使它成为在两种语言之间进行翻译的理想选择。\n",
    "\n",
    "Consider the sentence `Je ne suis pas le chat noir` →\n",
    "`I am not the black cat`. Most of the words in the input sentence have a\n",
    "direct translation in the output sentence, but are in slightly different\n",
    "orders, e.g. `chat noir` and `black cat`. Because of the `ne/pas`\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "例如句子`Je ne suis pas le chat noir` → `I am not the black cat`。输入句子中的大多数单词在输出句子中都有直接翻译，但顺序稍有不同，例如`chat noir`和`black cat`。由于`ne/pas`结构，输入句子中还有一个额外的单词。直接从输入单词的序列中生成正确的翻译会很困难。\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \\\"meaning\\\" of the input sequence into a single\n",
    "vector --- a single point in some N dimensional space of sentences.\n",
    "\n",
    "使用seq2seq模型时，编码器会创建一个向量，在理想情况下，该向量将输入序列的“意义”编码成一个单一的向量——在某些N维句子空间中的一个点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "===========\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "seq2seq网络的编码器是一个RNN，它为输入句子的每个单词输出一些值。对于每个输入单词，编码器输出一个向量和一个隐藏状态，并使用该隐藏状态作为下一个输入单词的输入。\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/encoder-network.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hidden_size = 128\n",
    "# batch_size = 32\n",
    "# input_size = input_lang.n_words = 4601\n",
    "# output_size = output_lang.n_words = 2991\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # nn.Embedding(input_size, hidden_size) 创建一个输入大小为 input_size，输出大小为 hidden_size 的嵌入层\n",
    "        # input_size 是输入词汇表的大小，hidden_size 是嵌入向量的大小\n",
    "        # nn.Embedding 会将输入的整数索引转换为大小为 hidden_size 的嵌入向量\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input: (batch_size, seq_len)= (32, 10)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # input的第二维里面的每个元素都是一个整数索引，表示词汇表中的单词索引, embedding(input)会查找到对应的嵌入向量\n",
    "        # 如第二维的某个元素值为i, 则embedding(i of input): (input_size, hidden_size)[i]: (hidden_size)找到第i个(第i行的, 这个[i]我这里表示相当于数组取第i行)嵌入向量 \n",
    "        # self.embedding(input): (batch_size, seq_len, hidden_size)= (32, 10, 128)\n",
    "        # embedded: (batch_size, seq_len, hidden_size)= (32, 10, 128)\n",
    "        output, hidden = self.gru(embedded)\n",
    "        # output: (batch_size, seq_len, hidden_size)= (32, 10, 128)\n",
    "        # hidden: (num_layers, batch_size, hidden_size)= (1, 32, 128)\n",
    "        return output, hidden\n",
    "        # output: (batch_size, seq_len, hidden_size)= (32, 10, 128)\n",
    "        # hidden: (num_layers, batch_size, hidden_size)= (1, 32, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4601\n",
      "2991\n"
     ]
    }
   ],
   "source": [
    "print(input_lang.n_words)\n",
    "print(output_lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.0809, -0.2764, -0.6606],\n",
      "        [-0.9478, -0.9776,  1.4272],\n",
      "        [-0.3416,  0.4340,  1.1769],\n",
      "        [-1.6055,  0.8642, -0.8805],\n",
      "        [ 1.4900, -1.0527, -0.2781],\n",
      "        [-0.7973,  0.4130, -0.0401],\n",
      "        [ 2.2093,  1.1120,  0.7554],\n",
      "        [ 0.0854, -0.9074, -0.6337],\n",
      "        [ 0.0219, -1.4945,  0.6451],\n",
      "        [ 0.0122,  0.9897, -0.1523]], requires_grad=True)\n",
      "tensor([[[-0.9478, -0.9776,  1.4272],\n",
      "         [-0.3416,  0.4340,  1.1769],\n",
      "         [ 1.4900, -1.0527, -0.2781],\n",
      "         [-0.7973,  0.4130, -0.0401]],\n",
      "\n",
      "        [[ 1.4900, -1.0527, -0.2781],\n",
      "         [-1.6055,  0.8642, -0.8805],\n",
      "         [-0.3416,  0.4340,  1.1769],\n",
      "         [ 0.0122,  0.9897, -0.1523]]], grad_fn=<EmbeddingBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 1.8191, -1.3784, -0.3600],\n",
      "        [-0.5796, -0.8333,  0.0368],\n",
      "        [-0.4686,  0.7916,  0.9450],\n",
      "        [ 0.2114,  1.1717,  1.6877],\n",
      "        [ 1.6693,  0.4422, -1.9951],\n",
      "        [-0.9450,  1.1735, -0.0832],\n",
      "        [-0.5706,  0.8138, -1.4372],\n",
      "        [-0.3925,  0.3512, -0.1663],\n",
      "        [-0.4198, -1.0016, -0.1003]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "         [-0.5796, -0.8333,  0.0368],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 1.6693,  0.4422, -1.9951]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 一个包含10个大小为3的张量的嵌入模块\n",
    "embedding = nn.Embedding(10, 3)\n",
    "# 一个包含2个样本的批次，每个样本有4个索引\n",
    "print(embedding.weight)\n",
    "input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
    "print(embedding(input))\n",
    "\n",
    "embedding = nn.Embedding(10, 3, padding_idx=0)\n",
    "# padding_idx=0 表示索引为 0 的嵌入向量将始终为零\n",
    "# 这是合理的, 因为一般来说padding一个input的时候, 会用0来填充\n",
    "input = torch.LongTensor([[0, 2, 0, 5]])\n",
    "print(embedding.weight)\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "===========\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "解码器是另一个RNN，它接收编码器输出的向量，并输出一个单词序列来生成翻译。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Decoder\n",
    "==============\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder.\n",
    "This last output is sometimes called the *context vector* as it encodes\n",
    "context from the entire sequence. This context vector is used as the\n",
    "initial hidden state of the decoder.\n",
    "\n",
    "在最简单的seq2seq解码器中，我们仅使用编码器的最后一个输出。这个最后的输出有时被称为*上下文向量*，因为它编码了整个序列的上下文。这个上下文向量用作解码器的初始隐藏状态。\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and\n",
    "hidden state. The initial input token is the start-of-string `<SOS>`\n",
    "token, and the first hidden state is the context vector (the encoder\\'s\n",
    "last hidden state).\n",
    "\n",
    "在解码的每一步，解码器都会接收一个输入标记和一个隐藏状态。初始输入标记是字符串开头的`<SOS>`标记，第一个隐藏状态是上下文向量（编码器的最后一个隐藏状态）。\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/decoder-network.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hidden_size = 128\n",
    "# batch_size = 32\n",
    "# input_size = input_lang.n_words = 4601\n",
    "# output_size = output_lang.n_words = 2991\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)= (32, 10, 128)\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        # decoder_input: (batch_size, 1)= (32, 1)\n",
    "        # torch.empty(batch_size, 1, dtype=torch.long, device=device): (batch_size, 1)\n",
    "        # fill_(SOS_token): 将所有元素填充为 SOS_token= 0\n",
    "        # decoder_input: (batch_size, 1) = (32, 1)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        # decoder_hidden: (1, batch_size, hidden_size)= (1, 32, 128)\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            # seq_len = MAX_LENGTH = 10\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            # decoder_output: (batch_size, 1, output_size)= (32, 1, 2991)\n",
    "            # decoder_hidden: (num_layers, batch_size, hidden_size)= (1, 32, 128)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            # decoder_outputs: [(batch_size, 1, output_size), ...]= [(32, 1, 2991), ...]\n",
    "            # decoder_outputs = [\n",
    "            #     tensor1,  # 形状为 (32, 1, 2991)\n",
    "            #     tensor2,  # 形状为 (32, 1, 2991)\n",
    "            #     tensor3,  # 形状为 (32, 1, 2991)\n",
    "            #     ...\n",
    "            #     tensor10  # 形状为 (32, 1, 2991)\n",
    "            # ]\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "                # target_tensor: (batch_size, seq_len) = (32, 10)\n",
    "                # target_tensor[:, i]: (batch_size,) = (32,)\n",
    "                # target_tensor[:, i].unsqueeze(1): (batch_size, 1)= (32, 1)\n",
    "                # unsqueeze(dim) 会在维度 dim 上插入一个新的维度\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                # decoder_output.topk(1) 返回两个张量：第一个是具有最高概率的值，第二个是对应的索引（即预测的单词索引）\n",
    "                # _: (batch_size, 1, 1)= (32, 1, 1)\n",
    "                # topi: (batch_size, 1, 1)= (32, 1, 1)\n",
    "                \n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "                # decoder_output.topk(1): 返回每行最大的元素值和索引\n",
    "                # squeeze(-1) 会去掉最后一个维度，使 topi 的形状从 (32, 1, 1) 变为 (32, 1)\n",
    "                # topi.squeeze(-1): (batch_size, 1)= (32, 1)\n",
    "                # detach() 会返回一个新的张量，从当前计算图中分离下来, 这在自回归生成中很重要，因为我们不希望当前时间步的预测影响前一步的梯度计算。\n",
    "                \n",
    "                \n",
    "        # decoder_outputs: 10 x (batch_size, 1, output_size)\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        # decoder_outputs: (batch_size, MAX_LENGTH, output_size)= (32, 10, 2991)\n",
    "        # torch.cat(decoder_outputs, dim=1): 沿着第二维度拼接\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        # decoder_outputs: (batch_size, seq_len, output_size)= (32, 10, 2991)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "        # decoder_outputs: (batch_size, seq_len, output_size)= (32, 10, 2991)\n",
    "        # decoder_hidden: (num_layers, batch_size, hidden_size)= (1, 32, 128)\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        # input: (batch_size, 1) = (32, 1)\n",
    "        # hidden: (num_layers, batch_size, hidden_size)= (1, 32, 128)\n",
    "        output = self.embedding(input)\n",
    "        # output: (batch_size, 1, hidden_size)= (32, 1, 128)\n",
    "        output = F.relu(output)\n",
    "        # output: (batch_size, 1, hidden_size)= (32, 1, 128)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # output: (batch_size, 1, hidden_size)= (32, 1, 128)\n",
    "        # hidden: (num_layers, batch_size, hidden_size)= (1, 32, 128)\n",
    "        output = self.out(output)\n",
    "        # output: (batch_size, 1, output_size)= (32, 1, 2991)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to train and observe the results of this model, but to\n",
    "save space we\\'ll be going straight for the gold and introducing the\n",
    "Attention Mechanism.\n",
    "\n",
    "我鼓励你训练并观察这个模型的结果，但为了节省空间，我们将直接介绍更先进的注意力机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Decoder\n",
    "=================\n",
    "\n",
    "If only the context vector is passed between the encoder and decoder,\n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "如果在编码器和解码器之间只传递上下文向量，那么这个单一的向量就需要承载编码整个句子的负担。\n",
    "\n",
    "Attention allows the decoder network to \\\"focus\\\" on a different part of\n",
    "the encoder\\'s outputs for every step of the decoder\\'s own outputs.\n",
    "First we calculate a set of *attention weights*. These will be\n",
    "multiplied by the encoder output vectors to create a weighted\n",
    "combination. The result (called `attn_applied` in the code) should\n",
    "contain information about that specific part of the input sequence, and\n",
    "thus help the decoder choose the right output words.\n",
    "\n",
    "注意力机制允许解码器网络在生成每一步输出时“聚焦”到编码器输出的不同部分。首先，我们计算一组*注意力权重*。这些权重将乘以编码器的输出向量以创建加权组合。结果（在代码中称为`attn_applied`）应该包含关于输入序列特定部分的信息，从而帮助解码器选择正确的输出单词。\n",
    "\n",
    "![](https://i.imgur.com/1152PYf.png)\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer `attn`, using the decoder\\'s input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    "计算注意力权重是通过另一个前馈层`attn`完成的，使用解码器的输入和隐藏状态作为输入。由于训练数据中包含各种长度的句子，为了实际创建和训练这一层，我们必须选择一个最大句子长度（即编码器输出的输入长度）来应用注意力权重。最大长度的句子将使用所有的注意力权重，而较短的句子将只使用前几个权重。\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/attention-decoder-network.png)\n",
    "\n",
    "Bahdanau attention, also known as additive attention, is a commonly used\n",
    "attention mechanism in sequence-to-sequence models, particularly in\n",
    "neural machine translation tasks. It was introduced by Bahdanau et al.\n",
    "in their paper titled [Neural Machine Translation by Jointly Learning to\n",
    "Align and Translate](https://arxiv.org/pdf/1409.0473.pdf). This\n",
    "attention mechanism employs a learned alignment model to compute\n",
    "attention scores between the encoder and decoder hidden states. It\n",
    "utilizes a feed-forward neural network to calculate alignment scores.\n",
    "\n",
    "Bahdanau注意力机制，也称为加性注意力，是序列到序列模型中常用的注意力机制，特别是在神经机器翻译任务中。它由Bahdanau等人在其论文[通过联合学习对齐和翻译进行神经机器翻译](https://arxiv.org/pdf/1409.0473.pdf)中提出。该注意力机制使用一个学习到的对齐模型来计算编码器和解码器隐藏状态之间的注意力得分。它利用一个前馈神经网络来计算对齐得分。\n",
    "\n",
    "However, there are alternative attention mechanisms available, such as\n",
    "Luong attention, which computes attention scores by taking the dot\n",
    "product between the decoder hidden state and the encoder hidden states.\n",
    "It does not involve the non-linear transformation used in Bahdanau\n",
    "attention.\n",
    "\n",
    "然而，还有其他可用的注意力机制，例如Luong注意力。Luong注意力通过计算解码器隐藏状态和编码器隐藏状态之间的点积来计算注意力得分。它不涉及Bahdanau注意力中使用的非线性变换。\n",
    "\n",
    "In this tutorial, we will be using Bahdanau attention. However, it would\n",
    "be a valuable exercise to explore modifying the attention mechanism to\n",
    "use Luong attention.\n",
    "\n",
    "在本教程中，我们将使用Bahdanau注意力。然而，探索修改注意力机制以使用Luong注意力将是一个有价值的练习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        # nn.Linear 包含两个主要参数：\n",
    "        # weight：权重矩阵，形状为 (out_features, in_features)。\n",
    "        # bias：偏置向量，形状为 (out_features,)。\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.Wa.weight: (hidden_size, hidden_size)= (128, 128)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.Ua.weight: (hidden_size, hidden_size)= (128, 128)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "        # self.Va.weight: (1, hidden_size)= (1, 128)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        # query: (batch_size, 1, hidden_size)= (32, 1, 128)\n",
    "        # keys: (batch_size, seq_len, hidden_size)= (32, 10, 128)\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        # Q = query · W_a.T + b_a= self.Wa(query): (batch_size, 1, hidden_size)= (32, 1, 128)\n",
    "        # K = keys · U_a.T + b_a= self.Ua(keys): (batch_size, seq_len, hidden_size)= (32, 10, 128)\n",
    "        # Q + K = self.Wa(query) + self.Ua(keys): (batch_size, seq_len, hidden_size) = (32, 10, 128)\n",
    "        # 这两个张量相加时会广播 query，使其与 keys 具有相同的形状，结果的形状为 (batch_size, seq_length, hidden_size)\n",
    "        # E = tanh(Q + K) = torch.tanh(self.Wa(query) + self.Ua(keys)): (batch_size, seq_len, hidden_size)= (32, 10, 128)\n",
    "        # scores = E · V_a.T + b_a = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys))): (batch_size, seq_len, 1)= (32, 10, 1)\n",
    "        \n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        # scores.squeeze(2): (batch_size, seq_len)= (32, 10)\n",
    "        # scores.squeeze(2).unsqueeze(1): (batch_size, 1, seq_len)= (32, 1, 10)\n",
    "        # scores: (batch_size, 1, seq_len) <- (batch_size, seq_len) <- (batch_size, seq_len, 1)\n",
    "        \n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        # weights: (batch_size, 1, seq_len) = (32, 1, 10)\n",
    "        \n",
    "        # keys: (batch_size, seq_len, hidden_size) = (32, 10, 128)\n",
    "        context = torch.bmm(weights, keys)\n",
    "        # context: (batch_size, 1, hidden_size)= (32, 1, 128) <- (32, 1, 10) x (32, 10, 128)\n",
    "        # bmm:  第一个张量的形状为 (batch_size, n, m)\n",
    "        #       第二个张量的形状为 (batch_size, m, p)\n",
    "        #       输出张量的形状为 (batch_size, n, p)\n",
    "\n",
    "        return context, weights\n",
    "        # context: (batch_size, 1, hidden_size)= (32, 1, 128)\n",
    "        # weights: (batch_size, 1, seq_len)= (32, 1, 10)\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        # hidden_size = 128\n",
    "        # output_size = 2991\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        # 调用父类nn.Module的初始化器\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # nn.Embedding(output_size, hidden_size) 创建一个输出大小为 output_size，输出大小为 hidden_size 的嵌入层\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        # input_size 是 2 * hidden_size，因为 input_gru 是拼接了嵌入向量和上下文向量\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)= (32, 10, 128)\n",
    "        # encoder_hidden: (1, batch_size, hidden_size)= (1, 32, 128)\n",
    "        # target_tensor: (batch_size, seq_len)= (32, 10)\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        # decoder_input: (batch_size, 1) = (32, 1)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        # decoder_hidden: (1, batch_size, hidden_size)= (1, 32, 128)\n",
    "        decoder_outputs = []\n",
    "        # decoder_outputs: [(batch_size, 1, output_size), ...]= [(32, 1, 2991), ...]\n",
    "        attentions = []\n",
    "        # attentions: [(batch_size, 1, seq_len), ...]= [(32, 1, 10), ...]\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            # seq_len = MAX_LENGTH = 10\n",
    "            # decoder_input: (batch_size, 1) = (32, 1)\n",
    "            # decoder_hidden: (1, batch_size, hidden_size) = (1, 32, 128)\n",
    "            # encoder_outputs: (batch_size, seq_len, hidden_size) = (32, 10, 128)\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # decoder_output: (batch_size, 1, output_size)= (32, 1, 2991)\n",
    "            # decoder_hidden: (num_layers, batch_size, hidden_size)= (1, 32, 128)\n",
    "            # attn_weights: (batch_size, 1, seq_len)= (32, 1, 10)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            # decoder_outputs: [(batch_size, 1, output_size), ...]= [(32, 1, 2991), ...]\n",
    "            attentions.append(attn_weights)\n",
    "            # attentions: [(batch_size, 1, seq_len), ...]= [(32, 1, 10), ...]\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "                # target_tensor: (batch_size, seq_len)= (32, 10)\n",
    "                # target_tensor[:, i]: (batch_size,)= (32,)\n",
    "                # target_tensor[:, i].unsqueeze(1): (batch_size, 1) = (32, 1)\n",
    "                # decoder_input: (batch_size, 1)= (32, 1)\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                # decoder_output.topk(1) 返回两个张量：第一个是具有最高概率的值，第二个是对应的索引（即预测的单词索引）\n",
    "                # _: (batch_size, 1, 1) = (32, 1, 1)\n",
    "                # topi: (batch_size, 1, 1) = (32, 1, 1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "                # decoder_output.topk(1): 返回每行最大的元素值和索引\n",
    "                # squeeze(-1) 会去掉最后一个维度，使 topi 的形状从 (32, 1, 1) 变为 (32, 1)\n",
    "                # topi.squeeze(-1): (batch_size, 1) = (32, 1)\n",
    "                # detach() 会返回一个新的张量，从当前计算图中分离下来, 这在自回归生成中很重要，因为我们不希望当前时间步的预测影响前一步的梯度计算\n",
    "                # decoder_input: (batch_size, 1) = (32, 1)\n",
    "\n",
    "        # seq_len = MAX_LENGTH = 10\n",
    "        # decoder_outputs: seq_len x (batch_size, 1, output_size)\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        # torch.cat(decoder_outputs, dim=1): 沿着第二维度拼接\n",
    "        # decoder_outputs: (batch_size, seq_len, output_size)= (32, 10, 2991)\n",
    "        \n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        # decoder_outputs: (batch_size, seq_len, output_size)= (32, 10, 2991)\n",
    "        # F.log_softmax(decoder_outputs, dim=-1): 沿着最后一个维度计算对数 softmax\n",
    "        \n",
    "        # attentions: seq_len x (batch_size, 1, seq_len)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "        # attentions: (batch_size, seq_len, seq_len)= (32, 10, 10)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "        # decoder_outputs: (batch_size, seq_len, output_size)= (32, 10, 2991)\n",
    "        # decoder_hidden: (num_layers, batch_size, hidden_size)= (1, 32, 128)\n",
    "        # attentions: (batch_size, seq_len, seq_len)= (32, 10, 10)\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        # input: (batch_size, 1) = (32, 1)\n",
    "        # hidden: (num_layers, batch_size, hidden_size)= (1, 32, 128)\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)= (32, 10, 128)\n",
    "        \n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "        # input的第二维里面的每个元素都是一个整数索引，表示词汇表中的单词索引, embedding(input)会查找到对应的嵌入向量\n",
    "        # 如第二维的某个元素值为i, 则embedding(i of input): (ouput_size, hidden_size)[i]: (hidden_size)找到第i个(第i行的, 这个[i]我这里表示相当于数组取第i行)嵌入向量\n",
    "        # self.embedding(input): (batch_size, 1, hidden_size)= (32, 1, 128)\n",
    "        \n",
    "        # hidden: (num_layers, batch_size, hidden_size)= (1, 32, 128)\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        # query = hidden.permute(1, 0, 2): (batch_size, num_layers, hidden_size)= (32, 1, 128)\n",
    "        \n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size) = (32, 10, 128)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        # context: (batch_size, 1, hidden_size)= (32, 1, 128)\n",
    "        # attn_weights: (batch_size, 1, seq_len)= (32, 1, 10)\n",
    "        \n",
    "        # embedded: (batch_size, 1, hidden_size)= (32, 1, 128)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "        # torch.cat((embedded, context), dim=2): 沿着第三维度拼接\n",
    "        # input_gru: (batch_size, 1, 2 * hidden_size)= (32, 1, 256)\n",
    "\n",
    "        # hidden: (num_layers, batch_size, hidden_size)= (1, 32, 128)\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        # output: (batch_size, 1, hidden_size)= (32, 1, 128)\n",
    "        # hidden: (num_layers, batch_size, hidden_size)= (1, 32, 128)\n",
    "        \n",
    "        output = self.out(output)\n",
    "        # output: (batch_size, 1, output_size)= (32, 1, 2991)\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "        # output: (batch_size, 1, output_size)= (32, 1, 2991)\n",
    "        # hidden: (num_layers, batch_size, hidden_size)= (1, 32, 128)\n",
    "        # attn_weights: (batch_size, 1, seq_len)= (32, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p>There are other forms of attention that work around the lengthlimitation by using a relative position approach. Read about \"localattention\" in <a href=\"https://arxiv.org/abs/1508.04025\">Effective Approaches to Attention-based Neural MachineTranslation</a>.</p>\n",
    "</div>\n",
    "\n",
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "为了训练，对于每一对句子，我们需要一个输入张量（输入句子中单词的索引）和一个目标张量（目标句子中单词的索引）。在创建这些向量时，我们会在两个序列的末尾添加EOS标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model 训练模型\n",
    "==================\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the `<SOS>` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "在训练过程中，我们将输入句子通过编码器，并记录每一个输出和最新的隐藏状态。然后，将 `<SOS>` 标记作为解码器的第一个输入，并将编码器的最后一个隐藏状态作为解码器的第一个隐藏状态。\n",
    "\n",
    "\\\"Teacher forcing\\\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder\\'s guess as the next\n",
    "input. Using teacher forcing causes it to converge faster but [when the\n",
    "trained network is exploited, it may exhibit\n",
    "instability](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf).\n",
    "\n",
    "“教师强制” 是指使用真实的目标输出作为每个下一个输入，而不是使用解码器的预测作为下一个输入。使用教师强制会使模型收敛更快，但 [当利用训练好的网络时，它可能会表现出不稳定性](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf)。\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation\n",
    "-intuitively it has learned to represent the output grammar and can\n",
    "\\\"pick up\\\" the meaning once the teacher tells it the first few words,\n",
    "but it has not properly learned how to create the sentence from the\n",
    "translation in the first place.\n",
    "\n",
    "你可以观察到教师强制网络的输出，其阅读具有连贯的语法但偏离正确的翻译——直观上它学会了表示输出语法，并且一旦教师告诉它前几个单词，它就能“拾取”意思，但它并没有正确学习如何从一开始就根据翻译创建句子。\n",
    "\n",
    "Because of the freedom PyTorch\\'s autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "`teacher_forcing_ratio` up to use more of it.\n",
    "\n",
    "由于 PyTorch 的 autograd 给了我们自由，我们可以通过一个简单的 if 语句随机选择是否使用教师强制。将 `teacher_forcing_ratio` 调高以更多地使用它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "        # print(input_tensor.size())\n",
    "        # print(target_tensor.size())\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        # print(encoder_outputs.size())\n",
    "        # print(encoder_hidden.size())\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n",
    "\n",
    "这是一个辅助函数，用于根据当前时间和进度百分比打印已用时间和预计剩余时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-   Start a timer\n",
    "-   Initialize optimizers and criterion\n",
    "-   Create set of training pairs\n",
    "-   Start empty losses array for plotting\n",
    "\n",
    "Then we call `train` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n",
    "\n",
    "整个训练过程如下：\n",
    "\n",
    "- 启动计时器\n",
    "- 初始化优化器和损失函数\n",
    "- 创建训练对集\n",
    "- 启动一个空的损失数组用于绘图\n",
    "\n",
    "然后我们多次调用 `train` 函数，并偶尔打印进度（例如的百分比、到目前为止的时间、预计时间）和平均损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results 绘制结果\n",
    "================\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "`plot_losses` saved while training.\n",
    "\n",
    "使用 matplotlib 进行绘图，使用在训练过程中保存的损失值数组 `plot_losses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder\\'s predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder\\'s\n",
    "attention outputs for display later.\n",
    "\n",
    "评估过程与训练过程大致相同，但没有目标值，因此我们只需在每一步将解码器的预测结果反馈给解码器本身。每次预测出一个词时，我们将其添加到输出字符串中，如果预测出 EOS 标记，我们就停止。同时，我们还存储解码器的注意力输出，以便稍后显示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but\n",
    "it makes it easier to run multiple experiments) we can actually\n",
    "initialize a network and start training.\n",
    "\n",
    "有了这些辅助函数（看起来是额外的工作，但它使运行多个实验更容易），我们实际上可以初始化网络并开始训练。\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small\n",
    "dataset we can use relatively small networks of 256 hidden nodes and a\n",
    "single GRU layer. After about 40 minutes on a MacBook CPU we\\'ll get\n",
    "some reasonable results.\n",
    "\n",
    "请记住，输入句子经过了大量过滤。对于这个小数据集，我们可以使用相对较小的网络，包括256个隐藏节点和一个GRU层。在MacBook CPU上运行大约40分钟后，我们将得到一些合理的结果。\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p>If you run this notebook you can train, interrupt the kernel,evaluate, and continue training later. Comment out the lines where theencoder and decoder are initialized and run <code>trainIters</code> again.</p>\n",
    "</div>\n",
    "\n",
    "如果你运行这个notebook，你可以训练、暂停内核、评估，然后继续训练。注释掉初始化编码器和解码器的行，然后再次运行 trainIters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n",
      "2991\n",
      "0m 28s (- 7m 14s) (5 6%) 1.5316\n",
      "0m 56s (- 6m 37s) (10 12%) 0.7015\n",
      "1m 24s (- 6m 6s) (15 18%) 0.3684\n",
      "1m 52s (- 5m 37s) (20 25%) 0.2049\n",
      "2m 20s (- 5m 10s) (25 31%) 0.1257\n",
      "2m 51s (- 4m 45s) (30 37%) 0.0866\n",
      "3m 19s (- 4m 16s) (35 43%) 0.0658\n",
      "3m 48s (- 3m 48s) (40 50%) 0.0539\n",
      "4m 17s (- 3m 20s) (45 56%) 0.0459\n",
      "5m 16s (- 3m 9s) (50 62%) 0.0409\n",
      "6m 15s (- 2m 50s) (55 68%) 0.0378\n",
      "7m 15s (- 2m 25s) (60 75%) 0.0348\n",
      "8m 14s (- 1m 54s) (65 81%) 0.0331\n",
      "9m 14s (- 1m 19s) (70 87%) 0.0312\n",
      "10m 13s (- 0m 40s) (75 93%) 0.0303\n",
      "11m 11s (- 0m 0s) (80 100%) 0.0289\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "# input_lang.n_words= 4601\n",
    "# output_lang.n_words= 2991\n",
    "print(output_lang.n_words)\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set dropout layers to `eval` mode\n",
    "\n",
    "将 dropout 层设置为 `eval` 模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je ne vais pas prendre le moindre risque\n",
      "= i m not taking any chances\n",
      "< i m not taking any chances <EOS>\n",
      "\n",
      "> j essaie d economiser de l argent\n",
      "= i m trying to save money\n",
      "< i m trying to save money <EOS>\n",
      "\n",
      "> nous ne sommes que des enfants\n",
      "= we re just children\n",
      "< we are sorry we children <EOS>\n",
      "\n",
      "> tu es tres attirant\n",
      "= you re very attractive\n",
      "< you re very attractive too much friend <EOS>\n",
      "\n",
      "> je suis aussi perplexe que tu l es\n",
      "= i m just as confused as you are\n",
      "< i m just as confused as you are <EOS>\n",
      "\n",
      "> nous sommes toujours prudents\n",
      "= we re always careful\n",
      "< we re always careful <EOS>\n",
      "\n",
      "> tu es merveilleuse\n",
      "= you re wonderful\n",
      "< you re wonderful <EOS>\n",
      "\n",
      "> je porte mon maillot de bain sous mes vetements\n",
      "= i m wearing my swimsuit under my clothes\n",
      "< i m wearing my swimsuit under my clothes <EOS>\n",
      "\n",
      "> tu es une petite menteuse\n",
      "= you re a little liar\n",
      "< you re a little liar <EOS>\n",
      "\n",
      "> nous sommes en retard\n",
      "= we re late\n",
      "< we re late <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Attention 可视化注意力机制\n",
    "=====================\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable\n",
    "outputs. Because it is used to weight specific encoder outputs of the\n",
    "input sequence, we can imagine looking where the network is focused most\n",
    "at each time step.\n",
    "\n",
    "注意力机制的一个有用特性是其高度可解释的输出。因为它用于对输入序列的特定编码器输出进行加权，我们可以想象在每个时间步网络最关注的地方。\n",
    "\n",
    "You could simply run `plt.matshow(attentions)` to see attention output\n",
    "displayed as a matrix. For a better viewing experience we will do the\n",
    "extra work of adding axes and labels:\n",
    "\n",
    "你可以简单地运行 `plt.matshow(attentions)` 来将注意力输出显示为一个矩阵。为了更好的观看体验，我们将额外添加坐标轴和标签："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = il n est pas aussi grand que son pere\n",
      "output = he is not as tall as his father <EOS>\n",
      "input = je suis trop fatigue pour conduire\n",
      "output = i m too tired to drive <EOS>\n",
      "input = je suis desole si c est une question idiote\n",
      "output = i m sorry if this is a stupid question <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5632/1690937169.py:8: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
      "/tmp/ipykernel_5632/1690937169.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + output_words)\n",
      "/tmp/ipykernel_5632/1690937169.py:8: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
      "/tmp/ipykernel_5632/1690937169.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + output_words)\n",
      "/tmp/ipykernel_5632/1690937169.py:8: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
      "/tmp/ipykernel_5632/1690937169.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + output_words)\n",
      "/tmp/ipykernel_5632/1690937169.py:8: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
      "/tmp/ipykernel_5632/1690937169.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + output_words)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = je suis reellement fiere de vous\n",
      "output = i m really proud of you yet <EOS>\n"
     ]
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n",
    "\n",
    "\n",
    "evaluateAndShowAttention('il n est pas aussi grand que son pere')\n",
    "\n",
    "evaluateAndShowAttention('je suis trop fatigue pour conduire')\n",
    "\n",
    "evaluateAndShowAttention('je suis desole si c est une question idiote')\n",
    "\n",
    "evaluateAndShowAttention('je suis reellement fiere de vous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises 练习\n",
    "=========\n",
    "\n",
    "-   Try with a different dataset\n",
    "    -   Another language pair\n",
    "    -   Human → Machine (e.g. IOT commands)\n",
    "    -   Chat → Response\n",
    "    -   Question → Answer\n",
    "-   Replace the embeddings with pretrained word embeddings such as\n",
    "    `word2vec` or `GloVe`\n",
    "-   Try with more layers, more hidden units, and more sentences. Compare\n",
    "    the training time and results.\n",
    "-   If you use a translation file where pairs have two of the same\n",
    "    phrase (`I am test \\t I am test`), you can use this as an\n",
    "    autoencoder. Try this:\n",
    "    -   Train as an autoencoder\n",
    "    -   Save only the Encoder network\n",
    "    -   Train a new Decoder for translation from there\n",
    "\n",
    "- 试试使用不同的数据集\n",
    "    - 另一种语言对\n",
    "    - 人类 → 机器（例如：物联网命令）\n",
    "    - 聊天 → 回复\n",
    "    - 问题 → 答案\n",
    "- 使用预训练的词嵌入（如 `word2vec` 或 `GloVe`）替换当前的嵌入\n",
    "- 尝试更多的层数、更多的隐藏单元和更多的句子。比较训练时间和结果。\n",
    "- 如果你使用一个翻译文件，其中对包含两个相同的短语（例如：`I am test \\t I am test`），你可以将其用作自动编码器。试试这个：\n",
    "    - 作为自动编码器进行训练\n",
    "    - 仅保存编码器网络\n",
    "    - 从那里训练一个新的解码器用于翻译"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
