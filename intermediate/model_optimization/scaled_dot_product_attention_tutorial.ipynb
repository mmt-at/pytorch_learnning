{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)\n",
    "\n",
    "实现高性能Transformer与缩放点积注意力（SDPA）\n",
    "\n",
    "==========================================================================================\n",
    "\n",
    "**Author:** [Driss Guessous](https://github.com/drisspg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary 摘要\n",
    "=======\n",
    "\n",
    "In this tutorial, we want to highlight a new `torch.nn.functional`\n",
    "function that can be helpful for implementing transformer architectures.\n",
    "The function is named\n",
    "`torch.nn.functional.scaled_dot_product_attention`. For detailed\n",
    "description of the function, see the [PyTorch\n",
    "documentation](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention).\n",
    "This function has already been incorporated into\n",
    "`torch.nn.MultiheadAttention` and `torch.nn.TransformerEncoderLayer`.\n",
    "\n",
    "在本教程中，我们将重点介绍一个新功能，该功能对于实现transformer架构非常有帮助。这个功能名为`torch.nn.functional.scaled_dot_product_attention`。有关该功能的详细说明，请参阅PyTorch文档。该功能已经被整合到`torch.nn.MultiheadAttention`和`torch.nn.TransformerEncoderLayer`中。\n",
    "\n",
    "\n",
    "Overview 概述\n",
    "========\n",
    "\n",
    "At a high level, this PyTorch function calculates the scaled dot product\n",
    "attention (SDPA) between query, key, and value according to the\n",
    "definition found in the paper [Attention is all you\n",
    "need](https://arxiv.org/abs/1706.03762). While this function can be\n",
    "written in PyTorch using existing functions, a fused implementation can\n",
    "provide large performance benefits over a naive implementation.\n",
    "\n",
    "从高层次上讲，这个PyTorch函数根据论文《Attention is all you need》中定义的方式计算查询、键和值之间的缩放点积注意力（SDPA）。虽然可以使用现有的PyTorch函数编写此功能，但融合实现可以比朴素实现提供更大的性能优势。\n",
    "\n",
    "Fused implementations 融合实现\n",
    "=====================\n",
    "\n",
    "For CUDA tensor inputs, the function will dispatch into one of the\n",
    "following implementations:\n",
    "\n",
    "-   [FlashAttention: Fast and Memory-Efficient Exact Attention with\n",
    "    IO-Awareness](https://arxiv.org/abs/2205.14135)\n",
    "-   [Memory-Efficient\n",
    "    Attention](https://github.com/facebookresearch/xformers)\n",
    "-   A PyTorch implementation defined in C++\n",
    "\n",
    "对于CUDA张量输入，函数将调度到以下实现之一：\n",
    "\n",
    "- FlashAttention: 快速且内存高效的IO感知精确注意力\n",
    "- 内存高效注意力\n",
    "- 用C++定义的PyTorch实现\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p>This tutorial requires PyTorch 2.0.0 or later.</p>\n",
    "</div>\n",
    "\n",
    "本教程需要PyTorch 2.0.0或更高版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Example Usage:\n",
    "query, key, value = torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device)\n",
    "F.scaled_dot_product_attention(query, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit Dispatcher Control 显式调度控制\n",
    "===========================\n",
    "\n",
    "While the function will implicitly dispatch to one of the three\n",
    "implementations, the user can also explicitly control the dispatch via\n",
    "the use of a context manager. This context manager allows users to\n",
    "explicitly disable certain implementations. If a user wants to ensure\n",
    "the function is indeed using the fastest implementation for their\n",
    "specific inputs, the context manager can be used to sweep through\n",
    "measuring performance.\n",
    "\n",
    "虽然该函数将隐式调度到三种实现之一，但用户也可以通过使用上下文管理器显式控制调度。该上下文管理器允许用户显式禁用某些实现。如果用户希望确保函数确实使用了其特定输入的最快实现，可以使用上下文管理器进行测量性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The default implementation runs in 1278.631 microseconds\n",
      "The default implementation runs in 1250.900 microseconds\n",
      "The math implementation runs in 7487.678 microseconds\n",
      "The flash attention implementation runs in 1240.477 microseconds\n",
      "The memory efficient implementation runs in 3143.190 microseconds\n"
     ]
    }
   ],
   "source": [
    "# Lets define a helpful benchmarking function:\n",
    "import torch.utils.benchmark as benchmark\n",
    "def benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n",
    "    t0 = benchmark.Timer(\n",
    "        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n",
    "    )\n",
    "    return t0.blocked_autorange().mean * 1e6\n",
    "\n",
    "# Lets define the hyper-parameters of our input\n",
    "batch_size = 32\n",
    "max_sequence_len = 1024\n",
    "num_heads = 32\n",
    "embed_dimension = 32\n",
    "\n",
    "dtype = torch.float16\n",
    "\n",
    "query = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
    "key = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
    "value = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
    "\n",
    "print(f\"The default implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n",
    "\n",
    "print(f\"The default implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n",
    "\n",
    "# Lets explore the speed of each of the 3 implementations\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "\n",
    "\n",
    "with sdpa_kernel(SDPBackend.MATH):\n",
    "    math_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n",
    "    print(f\"The math implementation runs in {math_time:.3f} microseconds\")\n",
    "\n",
    "with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "    try:\n",
    "        flash_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n",
    "        print(f\"The flash attention implementation runs in {flash_time:.3f} microseconds\")\n",
    "    except RuntimeError:\n",
    "        print(\"FlashAttention is not supported. See warnings for reasons.\")\n",
    "\n",
    "with sdpa_kernel(SDPBackend.EFFICIENT_ATTENTION):\n",
    "    try:\n",
    "        efficient_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n",
    "        print(f\"The memory efficient implementation runs in {efficient_time:.3f} microseconds\")\n",
    "    except RuntimeError:\n",
    "        print(\"EfficientAttention is not supported. See warnings for reasons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardware dependence 硬件依赖\n",
    "===================\n",
    "\n",
    "Depending on what machine you ran the above cell on and what hardware is\n",
    "available, your results might be different. - If you don't have a GPU\n",
    "and are running on CPU then the context manager will have no effect and\n",
    "all three runs should return similar timings. - Depending on what\n",
    "compute capability your graphics card supports flash attention or memory\n",
    "efficient might have failed.\n",
    "\n",
    "根据你运行上述代码的机器和可用的硬件，结果可能会有所不同。 - 如果没有GPU并且在CPU上运行，则上下文管理器不会产生任何效果，所有三次运行的时间应该相似。 - 根据你的显卡支持的计算能力，flash attention或内存高效可能会失败。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal Self Attention 因果自注意力\n",
    "=====================\n",
    "\n",
    "Below is an example implementation of a multi-headed causal self\n",
    "attention block inspired by [Andrej Karpathy\n",
    "NanoGPT](https://github.com/karpathy/nanoGPT) repository.\n",
    "\n",
    "下面是一个受Andrej Karpathy的NanoGPT库启发的多头因果自注意力模块的示例实现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalSelfAttention(\n",
      "  (c_attn): Linear(in_features=512, out_features=1536, bias=False)\n",
      "  (c_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads: int, embed_dimension: int, bias: bool=False, is_causal: bool=False, dropout:float=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dimension % num_heads == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(embed_dimension, 3 * embed_dimension, bias=bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(embed_dimension, embed_dimension, bias=bias)\n",
    "        # regularization\n",
    "        self.dropout = dropout\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dimension = embed_dimension\n",
    "        # Perform causal masking\n",
    "        self.is_causal = is_causal\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [x]: (batch_size, seq_length, embed_dimension)\n",
    "        \n",
    "        # <num_heads>= 8\n",
    "        # <heads_per_dim> = 64\n",
    "        \n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        \n",
    "        query_projected = self.c_attn(x)\n",
    "        # [query_projected]: (batch_size, seq_length, 3 * embed_dimension) <- [x]: (batch_size, seq_length, embed_dimension)\n",
    "        # [query_projected]: (32, 512, 3 * 512) <- (32, 512, 512)\n",
    "        # [query_projected]= (32, 512, 1536)\n",
    "\n",
    "        batch_size = query_projected.size(0)\n",
    "        # <batch_size>= (batch_size, seq_length, 3 * embed_dimension)(0) = batch_size\n",
    "        # <batch_size>= 32\n",
    "        \n",
    "        embed_dim = query_projected.size(2)\n",
    "        # <embed_dim>= 3 * embed_dimension= 1536\n",
    "        \n",
    "        head_dim = embed_dim // (self.num_heads * 3)\n",
    "        # <head_dim>= embed_dim // (num_heads * 3) = embed_dim // (8 * 3) = (3 * embed_dimension) // (8 * 3) \n",
    "        #           = embed_dimension // 8\n",
    "        #           = embed_dimension // num_heads\n",
    "        # <head_dim>= 64\n",
    "\n",
    "        query, key, value = query_projected.chunk(3, -1)\n",
    "        # [query]: (batch_size, seq_length, embed_dimension): (32, 512, 512)\n",
    "        # [key]: (batch_size, seq_length, embed_dimension): (32, 512, 512)\n",
    "        # [value]: (batch_size, seq_length, embed_dimension): (32, 512, 512)\n",
    "        # chunk(3, -1)：方法 chunk 会将张量分成指定数量的块。这里的 3 表示要分成3个块，-1 表示沿着最后一个维度进行分割\n",
    "        \n",
    "        query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n",
    "        # [query]: (batch_size, num_heads, seq_length, head_dim) <- (batch_size, seq_length, num_heads, head_dim) <- (batch_size, seq_length, embed_dimension)\n",
    "        # [query]: (32, 8, 512, 64) <- (32, 512, 8, 64) <- (32, 512, 512)\n",
    "        # view(batch_size, -1, self.num_heads, head_dim) 将 query 从 (batch_size, seq_length, embed_dimension) 转换为 (batch_size, seq_length, num_heads, head_dim), 其中 embed_dimension 被拆分为 num_heads 和 head_dim 两个维度, 即 embed_dimension = num_heads * head_dim\n",
    "        # transpose(1, 2) 交换了第二维和第三维的位置\n",
    "        \n",
    "        key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n",
    "        # [key]: (batch_size, num_heads, seq_length, head_dim) <- (batch_size, seq_length, num_heads, head_dim) <- (batch_size, seq_length, embed_dimension)\n",
    "        # [key]: (32, 8, 512, 64) <- (32, 512, 8, 64) <- (32, 512, 512)\n",
    "        \n",
    "        value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n",
    "        # [value]: (batch_size, num_heads, seq_length, head_dim) <- (batch_size, seq_length, num_heads, head_dim) <- (batch_size, seq_length, embed_dimension)\n",
    "        # [value]: (32, 8, 512, 64) <- (32, 512, 8, 64) <- (32, 512, 512)\n",
    "        \n",
    "        if self.training:\n",
    "            dropout = self.dropout\n",
    "            is_causal = self.is_causal\n",
    "        else:\n",
    "            dropout = 0.0\n",
    "            is_causal = False\n",
    "\n",
    "        y = F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=dropout, is_causal=is_causal)\n",
    "        # [y]: (batch_size, num_heads, seq_length, head_dim)\n",
    "        # [y]: (32, 8, 512, 64)\n",
    "        \n",
    "        # [query, key, value]: (batch_size, num_heads, seq_length, head_dim)= (32, 8, 512, 64)\n",
    "        \n",
    "        \n",
    "        y = y.transpose(1, 2).view(batch_size, -1, self.num_heads * head_dim)\n",
    "        # [y]: (batch_size, seq_length, embed_dimension) <- (batch_size, seq_length, num_heads, head_dim) <- (batch_size, num_heads, seq_length, head_dim)\n",
    "        # [y]: (batch_size, seq_length, embed_dimension)\n",
    "        \n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        # [y]: (batch_size, seq_length, embed_dimension)\n",
    "        return y\n",
    "\n",
    "\n",
    "num_heads = 8\n",
    "heads_per_dim = 64\n",
    "embed_dimension = num_heads * heads_per_dim\n",
    "# 512 = 8 * 64\n",
    "dtype = torch.float16\n",
    "model = CausalSelfAttention(num_heads=num_heads, embed_dimension=embed_dimension, bias=False, is_causal=True, dropout=0.1).to(\"cuda\").to(dtype).eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None) -> torch.Tensor:\n",
    "    # [query, key, value]: (batch_size, num_heads, seq_length, head_dim)= (32, 8, 512, 64)\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    # <L>= seq_length= 512\n",
    "    # <S>= seq_length= 512\n",
    "    \n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    # <query.size(-1)>= head_dim= 64\n",
    "    # <scale_factor>= 1 / sqrt(head_dim)= 1 / sqrt(64)= 1 / 8= 0.125\n",
    "    \n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "    # [attn_bias]: (seq_length, seq_length)= (512, 512)\n",
    "    \n",
    "    if is_causal:\n",
    "        assert attn_mask is None\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        # [temp_mask]: (seq_length, seq_length)= (512, 512)\n",
    "        # tril(diagonal=0)：返回一个矩阵，该矩阵是输入矩阵的下三角部分，其余部分被置零\n",
    "        \n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        # [attn_bias]: (seq_length, seq_length)= (512, 512)\n",
    "        # [temp_mask.logical_not()]: (seq_length, seq_length)= (512, 512)\n",
    "        # (4, 4)::\n",
    "        # Temp Mask:\n",
    "        # tensor([[ True, False, False, False],\n",
    "        #         [ True,  True, False, False],\n",
    "        #         [ True,  True,  True, False],\n",
    "        #         [ True,  True,  True,  True]])\n",
    "\n",
    "        # Attention Bias:\n",
    "        # tensor([[   0.,  -inf,  -inf,  -inf],\n",
    "        #         [   0.,    0.,  -inf,  -inf],\n",
    "        #         [   0.,    0.,    0.,  -inf],\n",
    "        #         [   0.,    0.,    0.,    0.]])\n",
    "        # temp_mask 是一个下三角布尔矩阵, 其中 True 表示可以注意到的位置, False 表示不能注意到的位置\n",
    "        # attn_bias 是一个下三角矩阵, 0 表示没有偏置, -inf 表示注意力分数将被极大地缩小, 从而有效地屏蔽了这些位置\n",
    "        \n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "            # [attn_bias]: (seq_length, seq_length)= (512, 512)\n",
    "            # Attention Mask:\n",
    "            # tensor([[ True, False,  True,  True],\n",
    "            #         [ True,  True, False,  True],\n",
    "            #         [ True,  True,  True, False],\n",
    "            #         [False,  True,  True,  True]])\n",
    "\n",
    "            # Attention Bias after masked_fill_:\n",
    "            # tensor([[   0., -inf,    0.,    0.],\n",
    "            #         [   0.,    0., -inf,    0.],\n",
    "            #         [   0.,    0.,    0., -inf],\n",
    "            #         [-inf,    0.,    0.,    0.]])\n",
    "            # attn_mask 是一个布尔张量，其中 True 表示可以注意到的位置，False 表示不能注意到的位置\n",
    "            # attn_mask.logical_not() 将 True 变为 False，False 变为 True\n",
    "            # attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\")) 将 attn_mask 中 False 的位置在 attn_bias 中填充为 -inf，这样在计算注意力权重时，这些位置的注意力分数将被极大地缩小，从而有效地屏蔽了这些位置\n",
    "                \n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "            # [attn_bias]: (seq_length, seq_length)= (512, 512)\n",
    "            # Attention Mask:\n",
    "            # tensor([[   0., -inf,    0.,    0.],\n",
    "            #         [   0.,    0., -inf,    0.],\n",
    "            #         [   0.,    0.,    0., -inf],\n",
    "            #         [-inf,    0.,    0.,    0.]])\n",
    "\n",
    "            # Attention Bias after adding attn_mask:\n",
    "            # tensor([[   0., -inf,    0.,    0.],\n",
    "            #         [   0.,    0., -inf,    0.],\n",
    "            #         [   0.,    0.,    0., -inf],\n",
    "            #         [-inf,    0.,    0.,    0.]])\n",
    "            # attn_mask 是一个包含具体偏置值的浮点张量，其中 0 表示没有偏置，-inf 表示该位置应该被屏蔽。\n",
    "            # attn_bias 初始为全零张量，形状为 (4, 4)\n",
    "            # attn_bias += attn_mask 将 attn_mask 中的值逐元素加到 attn_bias 中。结果是 attn_bias 的对应位置被更新为 attn_mask 的值。\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    # [attn_weight]: (batch_size, num_heads, seq_length_q, seq_length_k)= (32, 8, 512, 512)\n",
    "    \n",
    "    # [query]: (batch_size, num_heads, seq_length_q, head_dim)= (32, 8, 512, 64)\n",
    "    # [key]: (batch_size, num_heads, seq_length_k, head_dim)= (32, 8, 512, 64)\n",
    "    # [key.transpose(-2, -1)]: (batch_size, num_heads, head_dim, seq_length_k)= (32, 8, 64, 512)\n",
    "    \n",
    "    # [query @ key.transpose(-2, -1)]= [(batch_size, num_heads, seq_length_k, head_dim) @ (batch_size, num_heads, head_dim, seq_length_k)]= (batch_size, num_heads, seq_length_q, seq_length_k)= [(32, 8, 512, 64) @ (32, 8, 64, 512)]= (32, 8, 512, 512)\n",
    "    \n",
    "    attn_weight += attn_bias\n",
    "    # [attn_weight]: (batch_size, num_heads, seq_length_q, seq_length_k)= (32, 8, 512, 512)\n",
    "    # [attn_bias]: (seq_length_q, seq_length_k)= (512, 512)\n",
    "    # attn_weight += attn_bias 将 attn_bias 加到 attn_weight 上，这样在计算 softmax 时，这些位置的注意力分数将被极大地缩小，从而有效地屏蔽了这些位置\n",
    "    \n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    # [attn_weight]: (batch_size, num_heads, seq_length_q, seq_length_k)= (32, 8, 512, 512)\n",
    "    # softmax(dim=-1)：对最后一个维度进行 softmax 操作，即对 seq_length_k 进行 softmax 操作\n",
    "    # 未应用 softmax 前，attn_weight 包含未归一化的注意力得分。\n",
    "    # softmax 操作将所有位置的注意力分数归一化到 [0, 1] 之间，使得所有位置的注意力分数之和为 1\n",
    "    # 应用 softmax 后，attn_weight 变成了概率分布，每个查询位置对于所有键位置的注意力权重和为1。\n",
    "    \n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    # [attn_weight]: (batch_size, num_heads, seq_length, seq_length)= (32, 8, 512, 512)\n",
    "    # dropout(attn_weight, dropout_p, train=True)：对 attn_weight 进行 dropout 操作，以减少过拟合\n",
    "    # dropout_p=0.0 表示没有 dropout 操作\n",
    "    \n",
    "    return attn_weight @ value\n",
    "    # [value]: (batch_size, num_heads, seq_length_v, head_dim)= (32, 8, 512, 64)\n",
    "    # [attn_weight @ value]: [(batch_size, num_heads, seq_length_q, seq_length_k) @ (batch_size, num_heads, seq_length_v, head_dim)]= (batch_size, num_heads, seq_length, head_dim)= (32, 8, 512, 64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NestedTensor` and Dense tensor support `NestedTensor`与Dense Tensor支持\n",
    "=======================================\n",
    "\n",
    "SDPA supports both `NestedTensor` and Dense tensor inputs.\n",
    "`NestedTensors` handle the case where the input is a batch of variable\n",
    "length sequences without needing to pad each sequence to the maximum\n",
    "length in the batch. For more information about `NestedTensors` see\n",
    "[torch.nested](https://pytorch.org/docs/stable/nested.html) and\n",
    "[NestedTensors\n",
    "Tutorial](https://pytorch.org/tutorials/prototype/nestedtensor.html).\n",
    "\n",
    "SDPA支持`NestedTensor`和Dense Tensor输入。`NestedTensor`处理输入是可变长度序列的批次的情况，而无需将每个序列填充到批次的最大长度。有关NestedTensors的更多信息，请参阅`torch.nested`和NestedTensors教程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random NT runs in 494.214 microseconds\n",
      "Random Dense runs in 374.631 microseconds\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def generate_rand_batch(\n",
    "    batch_size,\n",
    "    max_sequence_len,\n",
    "    embed_dimension,\n",
    "    pad_percentage=None,\n",
    "    dtype=torch.float16,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    if not pad_percentage:\n",
    "        return (\n",
    "            torch.randn(\n",
    "                batch_size,\n",
    "                max_sequence_len,\n",
    "                embed_dimension,\n",
    "                dtype=dtype,\n",
    "                device=device,\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        # torch.randn(batch_size, max_sequence_len, embed_dimension, dtype=dtype, device=device)：生成一个形状为 (batch_size, max_sequence_len, embed_dimension) 的张量\n",
    "        # 返回值：(张量, None)\n",
    "        \n",
    "    # Random sequence lengths\n",
    "    seq_len_list = [\n",
    "        int(max_sequence_len * (1 - random.gauss(pad_percentage, 0.01)))\n",
    "        for _ in range(batch_size)\n",
    "    ]\n",
    "    # random.gauss(pad_percentage, 0.01)：从均值为 pad_percentage，标准差为 0.01 的正态分布中采样一个随机数, 生成的随机数大部分在 pad_percentage 附近\n",
    "    # 1 - random.gauss(pad_percentage, 0.01) 生成的随机数大部分在 1 - pad_percentage 附近\n",
    "    # int(max_sequence_len * (1 - random.gauss(pad_percentage, 0.01)))：生成一个随机的序列长度，大部分在 max_sequence_len * (1 - pad_percentage) 附近\n",
    "    # Make random entry in the batch have max sequence length\n",
    "    seq_len_list[random.randint(0, batch_size - 1)] = max_sequence_len\n",
    "    # random.randint(0, batch_size - 1)：生成一个随机的整数，范围在 [0, batch_size - 1] 之间\n",
    "    # seq_len_list[random.randint(0, batch_size - 1)] = max_sequence_len 将随机的一个序列长度设置为 max_sequence_len\n",
    "    \n",
    "    return (\n",
    "        torch.nested.nested_tensor(\n",
    "            [\n",
    "                torch.randn(seq_len, embed_dimension,\n",
    "                            dtype=dtype, device=device)\n",
    "                for seq_len in seq_len_list\n",
    "            ]\n",
    "        ),\n",
    "        seq_len_list,\n",
    "    )\n",
    "    # [torch.randn(seq_len, embed_dimension, dtype=dtype, device=device) for seq_len in seq_len_list]：生成一个嵌套张量，其中每个张量的形状为 (seq_len, embed_dimension)\n",
    "    # torch.nested.nested_tensor：将一个嵌套的张量列表转换为嵌套张量\n",
    "    # 返回值：(嵌套张量, 序列长度列表)\n",
    "\n",
    "random_nt, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=0.5, dtype=dtype, device=device)\n",
    "random_dense, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=None, dtype=dtype, device=device)\n",
    "\n",
    "# Currently the fused implementations don't support ``NestedTensor`` for training\n",
    "model.eval()\n",
    "\n",
    "with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "    try:\n",
    "        print(f\"Random NT runs in {benchmark_torch_function_in_microseconds(model, random_nt):.3f} microseconds\")\n",
    "        print(f\"Random Dense runs in {benchmark_torch_function_in_microseconds(model, random_dense):.3f} microseconds\")\n",
    "    except RuntimeError:\n",
    "        print(\"FlashAttention is not supported. See warnings for reasons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SDPA with `torch.compile` 使用`torch.compile`的SDPA\n",
    "===============================\n",
    "\n",
    "With the release of PyTorch 2.0, a new feature called `torch.compile()`\n",
    "has been introduced, which can provide significant performance\n",
    "improvements over eager mode. Scaled dot product attention is fully\n",
    "composable with `torch.compile()`. To demonstrate this, let\\'s compile\n",
    "the `CausalSelfAttention` module using `torch.compile()` and observe the\n",
    "resulting performance improvements.\n",
    "\n",
    "随着PyTorch 2.0的发布，一个名为`torch.compile()`的新特性被引入，该特性相较于即时模式可以提供显著的性能提升。缩放点积注意力可以完全与`torch.compile()`结合使用。为了演示这一点，我们将编译`CausalSelfAttention`模块，并观察由此带来的性能提升。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The non compiled module runs in  188.795 microseconds\n",
      "The compiled module runs in  286.094 microseconds\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_sequence_len = 256\n",
    "x = torch.rand(batch_size, max_sequence_len,\n",
    "               embed_dimension, device=device, dtype=dtype)\n",
    "print(\n",
    "    f\"The non compiled module runs in  {benchmark_torch_function_in_microseconds(model, x):.3f} microseconds\")\n",
    "\n",
    "\n",
    "compiled_model = torch.compile(model)\n",
    "# Let's compile it\n",
    "compiled_model(x)\n",
    "print(\n",
    "    f\"The compiled module runs in  {benchmark_torch_function_in_microseconds(compiled_model, x):.3f} microseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact execution time is dependent on machine, however the results\n",
    "for mine: The non compiled module runs in 166.616 microseconds The\n",
    "compiled module runs in 166.726 microseconds That is not what we were\n",
    "expecting. Let\\'s dig a little deeper. PyTorch comes with an amazing\n",
    "built-in profiler that you can use to inspect the performance\n",
    "characteristics of your code.\n",
    "\n",
    "确切的执行时间依赖于机器，但我的结果如下：非编译模块运行时间：166.616微秒，编译模块运行时间：166.726微秒。结果并不是我们所期望的。让我们深入探讨一下。PyTorch自带了一个惊人的内置分析器，可以用来检查代码的性能特征。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         Non-Compilied Causal Attention         0.00%       0.000us         0.00%       0.000us       0.000us       8.605ms        59.57%       8.605ms       8.605ms             1  \n",
      "                         Non-Compilied Causal Attention        18.39%       2.551ms        99.94%      13.859ms      13.859ms       0.000us         0.00%       5.839ms       5.839ms             1  \n",
      "                                           aten::matmul         2.10%     291.000us        54.64%       7.578ms     151.560us       0.000us         0.00%       4.217ms      84.340us            50  \n",
      "                                               aten::mm        30.77%       4.267ms        50.80%       7.045ms     140.900us       4.217ms        29.20%       4.217ms      84.340us            50  \n",
      "                                           aten::linear         1.59%     221.000us        57.83%       8.020ms     160.400us       0.000us         0.00%       4.114ms      82.280us            50  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize160x12...         0.00%       0.000us         0.00%       0.000us       0.000us       2.928ms        20.27%       2.928ms     117.120us            25  \n",
      "                     aten::scaled_dot_product_attention         1.76%     244.000us        15.84%       2.196ms      87.840us       0.000us         0.00%       1.622ms      64.880us            25  \n",
      "              aten::_scaled_dot_product_flash_attention         2.98%     413.000us        14.08%       1.952ms      78.080us       0.000us         0.00%       1.622ms      64.880us            25  \n",
      "                         aten::_flash_attention_forward         4.03%     559.000us        10.31%       1.430ms      57.200us       1.622ms        11.23%       1.622ms      64.880us            25  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us       1.622ms        11.23%       1.622ms      64.880us            25  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 13.868ms\n",
      "Self CUDA time total: 14.444ms\n",
      "\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                              Compiled Causal Attention         0.00%       0.000us         0.00%       0.000us       0.000us       8.701ms        59.78%       8.701ms       8.701ms             1  \n",
      "                              Compiled Causal Attention         8.88%       1.075ms        99.93%      12.104ms      12.104ms       0.000us         0.00%       5.853ms       5.853ms             1  \n",
      "                                  Torch-Compiled Region        10.62%       1.286ms        89.85%      10.883ms     435.320us       0.000us         0.00%       5.853ms     234.120us            25  \n",
      "                                       CompiledFunction        45.87%       5.556ms        78.12%       9.462ms     378.480us       0.000us         0.00%       5.853ms     234.120us            25  \n",
      "                                               aten::mm         9.40%       1.138ms        18.91%       2.290ms      45.800us       4.230ms        29.06%       4.230ms      84.600us            50  \n",
      "sm80_xmma_gemm_f16f16_f16f32_f32_tn_n_tilesize160x12...         0.00%       0.000us         0.00%       0.000us       0.000us       2.933ms        20.15%       2.933ms     117.320us            25  \n",
      "              aten::_scaled_dot_product_flash_attention         2.15%     261.000us        12.57%       1.523ms      60.920us       0.000us         0.00%       1.623ms      64.920us            25  \n",
      "                         aten::_flash_attention_forward         3.89%     471.000us         9.62%       1.165ms      46.600us       1.623ms        11.15%       1.623ms      64.920us            25  \n",
      "void pytorch_flash::flash_fwd_kernel<pytorch_flash::...         0.00%       0.000us         0.00%       0.000us       0.000us       1.623ms        11.15%       1.623ms      64.920us            25  \n",
      "ampere_fp16_s16816gemm_fp16_128x128_ldg8_f2f_stages_...         0.00%       0.000us         0.00%       0.000us       0.000us       1.297ms         8.91%       1.297ms      51.880us            25  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 12.112ms\n",
      "Self CUDA time total: 14.554ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-06-27 11:30:01 62177:62177 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-06-27 11:30:01 62177:62177 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-06-27 11:30:01 62177:62177 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "STAGE:2024-06-27 11:30:02 62177:62177 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-06-27 11:30:02 62177:62177 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-06-27 11:30:02 62177:62177 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "activities = [ProfilerActivity.CPU]\n",
    "if device == 'cuda':\n",
    "    activities.append(ProfilerActivity.CUDA)\n",
    "\n",
    "with profile(activities=activities, record_shapes=False) as prof:\n",
    "    with record_function(\" Non-Compilied Causal Attention\"):\n",
    "        for _ in range(25):\n",
    "            model(x)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "# 打印性能分析结果，按 CUDA 时间总和排序，显示前 10 行。\n",
    "\n",
    "with profile(activities=activities, record_shapes=False) as prof:\n",
    "    with record_function(\"Compiled Causal Attention\"):\n",
    "        for _ in range(25):\n",
    "            compiled_model(x)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "# For even more insights, you can export the trace and use ``chrome://tracing`` to view the results\n",
    "#\n",
    "# .. code-block:: python\n",
    "#\n",
    "#    prof.export_chrome_trace(\"compiled_causal_attention_trace.json\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code snippet generates a report of the top 10 PyTorch\n",
    "functions that consumed the most GPU execution time, for both the\n",
    "compiled and non-compiled module. The analysis reveals that the majority\n",
    "of time spent on the GPU is concentrated on the same set of functions\n",
    "for both modules. The reason for this here is that `torch.compile` is\n",
    "very good at removing the framework overhead associated with PyTorch. If\n",
    "your model is launching large, efficient CUDA kernels, which in this\n",
    "case `CausalSelfAttention` is, then the overhead of PyTorch can be\n",
    "hidden.\n",
    "\n",
    "先前的代码生成了一份报告，显示了消耗GPU执行时间最多的前十个PyTorch函数，分别针对编译和未编译模块。分析显示，GPU上大多数时间都集中在相同的一组函数上，这两个模块都是如此。原因是`torch.compile`非常擅长消除与PyTorch相关的框架开销。如果你的模型启动了大型、高效的CUDA内核（在这种情况下，`CausalSelfAttention`就是这种情况），那么PyTorch的开销可能会被隐藏。\n",
    "\n",
    "In reality, your module does not normally consist of a singular\n",
    "`CausalSelfAttention` block. When experimenting with [Andrej Karpathy\n",
    "NanoGPT](https://github.com/karpathy/nanoGPT) repository, compiling the\n",
    "module took the time per train step from: `6090.49ms` to `3273.17ms`!\n",
    "This was done on commit: `ae3a8d5` of NanoGPT training on the\n",
    "Shakespeare dataset.\n",
    "\n",
    "实际上，你的模块通常不会仅包含一个`CausalSelfAttention`块。在实验Andrej Karpathy的[NanoGPT](https://github.com/karpathy/nanoGPT)库时，编译模块将每次训练步骤的时间从6090.49毫秒减少到3273.17毫秒！这是在NanoGPT的commit: `ae3a8d5`上对Shakespeare数据集进行训练时实现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SDPA with attn\\_bias subclasses\\`\n",
    "=======================================\n",
    "\n",
    "As of PyTorch 2.3, we have added a new submodule that contains tensor\n",
    "subclasses. Designed to be used with\n",
    "`torch.nn.functional.scaled_dot_product_attention`. The module is named\n",
    "`torch.nn.attention.bias` and contains the following two utilities for\n",
    "generating causal attention variants:\n",
    "\n",
    "-   `torch.nn.attention.bias.causal_upper_left`\n",
    "-   `torch.nn.attention.bias.causal_lower_right`\n",
    "\n",
    "从PyTorch 2.3开始，我们添加了一个包含张量子类的新子模块。该模块设计用于与`torch.nn.functional.scaled_dot_product_attention`一起使用。这个模块名为`torch.nn.attention.bias`，包含以下两个生成因果注意力变体的工具：\n",
    "\n",
    "-   `torch.nn.attention.bias.causal_upper_left`\n",
    "-   `torch.nn.attention.bias.causal_lower_right`\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p>The current argument <code>is_causal</code> in <code>torch.nn.functional.scaled_dot_product_attention</code>is the same as using <code>torch.nn.attention.bias.causal_upper_left</code>.</p>\n",
    "</div>\n",
    "\n",
    "当前在`torch.nn.functional.scaled_dot_product_attention`中的参数`is_causal`与使用`torch.nn.attention.bias.causal_upper_left`相同。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.attention.bias.CausalBias'>\n",
      "<class 'torch.nn.attention.bias.CausalBias'>\n",
      "tensor([[ True, False, False, False, False, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False, False, False, False, False]])\n",
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.attention.bias import causal_lower_right, causal_upper_left\n",
    "\n",
    "batch_size = 32\n",
    "sequence_length_q = 2\n",
    "sequence_length_kv = 10\n",
    "num_heads = 16\n",
    "embed_dimension = 32\n",
    "\n",
    "dtype = torch.float16\n",
    "\n",
    "query = torch.rand(batch_size, num_heads, sequence_length_q, embed_dimension, device=device, dtype=dtype)\n",
    "key = torch.rand(batch_size, num_heads, sequence_length_kv, embed_dimension, device=device, dtype=dtype)\n",
    "value = torch.rand(batch_size, num_heads, sequence_length_kv, embed_dimension, device=device, dtype=dtype)\n",
    "\n",
    "upper_left_bias = causal_upper_left(sequence_length_q, sequence_length_kv)\n",
    "# [[ True, False, False, False, False, False, False, False, False, False],\n",
    "#  [ True,  True, False, False, False, False, False, False, False, False]]\n",
    "# 查询序列 q 中的第0个token只能注意到键序列 k 中的第0个token\n",
    "# 查询序列 q 中的第1个token可以注意到键序列 k 中的前2个token\n",
    "# 具体来说\n",
    "#   upper_left_bias[0] 表示查询序列 q 中的第0个token，对应键序列 k 的第0个token\n",
    "#   upper_left_bias[1] 表示查询序列 q 中的第1个token，对应键序列 k 的前2个token\n",
    "\n",
    "\n",
    "lower_right_bias = causal_lower_right(sequence_length_q, sequence_length_kv)\n",
    "# [[ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
    "#  [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]\n",
    "# 查询序列 q 中的第0个token可以注意到键序列 k 中的前9个token\n",
    "# 查询序列 q 中的第1个token可以注意到键序列 k 中的所有token\n",
    "# 具体来说\n",
    "#   lower_right_bias[0] 表示查询序列 q 中的第0个token，对应键序列 k 的前9个token\n",
    "#   lower_right_bias[1] 表示查询序列 q 中的第1个token，对应键序列 k 的所有token\n",
    "\n",
    "# 上左偏置 (causal_upper_left)：将因果注意力掩码对齐到注意力分数矩阵的左上角。适用于从头开始的解码场景。\n",
    "#   查询的第0个token只能关注到键的第0个token。\n",
    "#   查询的第1个token可以关注到键的第0个和第1个token。\n",
    "# 下右偏置 (causal_lower_right)：将因果注意力掩码对齐到注意力分数矩阵的右下角。适用于从尾开始的解码场景。\n",
    "#   查询的第0个token可以关注到键的前9个token。\n",
    "#   查询的第1个token可以关注到键的所有token。\n",
    "\n",
    "\n",
    "print(type(upper_left_bias))\n",
    "print(type(lower_right_bias))\n",
    "\n",
    "assert type(upper_left_bias) == type(lower_right_bias)\n",
    "assert issubclass(type(upper_left_bias), torch.Tensor)\n",
    "\n",
    "# As you can see from the previous output, are the same type ``torch.nn.attention.bias.CausalBias``\n",
    "# and subclass ``torch.Tensor``\n",
    "\n",
    "# Lets see what these tensors look like\n",
    "print(upper_left_bias)\n",
    "print(lower_right_bias)\n",
    "\n",
    "# Upper Left Bias aligns the causal attention mask to the upper left corner of the attention scores matrix.\n",
    "# This only has an impact when the attention scores matrix is not square, which is common for decoding use cases.\n",
    "# Another way of thinking about this concept is that when you use upper left bias,\n",
    "# the 0th token in the query is aligned to the 0th token in the key, while for lower right bias,\n",
    "# Assuming the attention score matrix is two dimensional, ``attn_score[0][0]`` is the attention score\n",
    "# between the 0th token in the query and the 0th token in the key.\n",
    "# For lower right bias, the sequence of q is aligned so that the last token in q is aligned to the last token in k\n",
    "# (for example, ``attn_score[-1][-1])`` is all True since the last token in q is at the same position as the last token in k\n",
    "# even if the sequence length of q and k are different.\n",
    "# Upper Left Bias将因果注意力掩码对齐到注意力分数矩阵的左上角\n",
    "# 当注意力分数矩阵不是方阵时，这只会产生影响，而这在解码用例中很常见\n",
    "# 另一种思考这个概念的方式是，当使用上左偏置时，查询中的第 0 个标记与键中的第 0 个标记对齐，而对于下右偏置，假设注意力分数矩阵是二维的，``attn_score[0][0]`` 是查询中第 0 个标记与键中第 0 个标记之间的注意力分数\n",
    "# 对于下右偏置，q 的序列对齐，使得 q 中的最后一个标记对齐到 k 中的最后一个标记（例如，``attn_score[-1][-1]`` 全为 True，因为 q 中的最后一个标记与 k 中的最后一个标记在同一位置，即使 q 和 k 的序列长度不同）\n",
    "\n",
    "# These objects are intended to be used with sdpa\n",
    "out_upper_left = F.scaled_dot_product_attention(query, key, value, upper_left_bias)\n",
    "out_lower_right = F.scaled_dot_product_attention(query, key, value, lower_right_bias)\n",
    "out_is_causal = F.scaled_dot_product_attention(query, key, value, is_causal=True)\n",
    "\n",
    "assert torch.allclose(out_upper_left, out_is_causal)\n",
    "assert not torch.allclose(out_upper_left, out_lower_right)\n",
    "\n",
    "# These attention biases should also be compatible with torch.compile\n",
    "compiled_sdpa = torch.compile(F.scaled_dot_product_attention, fullgraph=True)\n",
    "out_upper_left = compiled_sdpa(query, key, value, upper_left_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "==========\n",
    "\n",
    "In this tutorial, we have demonstrated the basic usage of\n",
    "`torch.nn.functional.scaled_dot_product_attention`. We have shown how\n",
    "the `sdpa_kernel` context manager can be used to assert a certain\n",
    "implementation is used on GPU. As well, we built a simple\n",
    "`CausalSelfAttention` module that works with `NestedTensor` and is torch\n",
    "compilable. In the process we have shown how to the profiling tools can\n",
    "be used to explore the performance characteristics of a user defined\n",
    "module.\n",
    "\n",
    "在本教程中，我们演示了`torch.nn.functional.scaled_dot_product_attention`的基本用法。我们展示了如何使用`sdpa_kernel`上下文管理器来断言在GPU上使用某种实现。此外，我们构建了一个简单的`CausalSelfAttention`模块，该模块支持`NestedTensor`并可编译。在此过程中，我们展示了如何使用分析工具来探索用户定义模块的性能特征。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
